<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data.">
  <meta name="keywords" content="LLaVA, stable diffusion, visual instruction tuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yanda Li<sup>1</sup>,</span>
            <span class="author-block">
              Chi Zhang<sup>2</sup>,</span>
            <span class="author-block">
              Gang Yu<sup>2</sup>,
            </span>
            <span class="author-block">
              Zhibin Wang<sup>2</sup>,
            </span>
            <span class="author-block">
              Bin Fu<sup>2</sup>,
            </span>
            <span class="author-block">
              Guosheng Lin<sup>3</sup>
            </span>
            <span class="author-block">
              Chunhua Shen<sup>4</sup>
            </span>
            <span class="author-block">
              Ling Chen<sup>1</sup>
            </span>
            <span class="author-block">
              Yunchao Wei<sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Technology Sydney</span>
            <span class="author-block"><sup>2</sup>Tencent</span>
            <span class="author-block"><sup>3</sup>Nanyang Technological University</span>
            <span class="author-block"><sup>4</sup>Zhejiang University</span>
            <span class="author-block"><sup>5</sup>Beijing Jiaotong University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2308.10253v1.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.10253v1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/icoz69/StableLLAVA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/icoz69/StableLLAVA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). 
           A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often 
            rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However,
            these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. 
            In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. 
            This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset 
            with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. 
            Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. 
            Our results underscore marked enhancements across more than ten commonly assessed capabilities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Teaser. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
            <div class="center-image">
              <figure>
                <img src="./static/images/teasor.png"
                   class="interpolation-image"
                   alt="Interpolate start reference image." />
                <figcaption>
                <strong>Examples of synthesized visual instruction data.</strong>
                We instruct ChatGPT to generate prompts for the text-to-image generative model,
                StableDiffusion, and a dialogue regarding the image content.
                The image-dialogue pairs are used to train the multimodal large language models.
              </figcaption>
            </figure>
            </div>
          </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). 
           A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often 
            rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However,
            these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. 
            In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. 
            This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset 
            with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. 
            Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. 
            Our results underscore marked enhancements across more than ten commonly assessed capabilities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Qualitative results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content has-text-justified">
          <p>
            Demonstrations of our method’s effectiveness across diverse real world image scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Qualitative results. -->

    <!-- Inserted Image -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <img src="case.png" alt="Case Image">
      </div>
    </div>
  </div>
</section>
  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2023stablellava,
  title={StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data},
  author={Li, Yanda and Zhang, Chi and Yu, Gang and Wang, Zhibin and Fu, Bin and Lin, Guosheng and Shen, Chunhua and Chen, Ling and Wei, Yunchao},
  journal={arXiv preprint arXiv:2308.10253},
  year={2023}
}</code></pre>
  </div>
</section>
  
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/stable_llava.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/icoz69/StableLLAVA" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://icoz69.github.io/stablellava-official">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
